{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  Tweet Topic Classification â€” Robust v3 (Colab Ready)\n",
        "\n",
        "This notebook trains a robust tweet topic classifier with:\n",
        "- Test-Time Augmentation (TTA)\n",
        "- Trimmed/bagged centroids or Mahalanobis\n",
        "- Calibrated Logistic Regression\n",
        "- kNN blending\n",
        "- Per-class open-set thresholds\n",
        "\n",
        "Artifacts are saved under `./artifacts_v3`. Run the setup cell once, then run all cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: install dependencies (Colab-safe)\n",
        "import sys, subprocess, importlib\n",
        "\n",
        "def ensure(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg.split(\"==\")[0].split(\">=\")[0])\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "for p in [\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"pandas>=2.0.0\",\n",
        "    \"numpy>=1.23.0\",\n",
        "    \"scikit-learn>=1.3.0\",\n",
        "    \"sentence-transformers>=2.2.2\",\n",
        "    \"emoji\",\n",
        "    \"wordsegment\",\n",
        "    \"matplotlib>=3.7.0\",\n",
        "    \"scipy>=1.10.0\",\n",
        "    \"joblib>=1.3.0\",\n",
        "]:\n",
        "    ensure(p)\n",
        "\n",
        "print(\"Dependencies ensured.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports & reproducibility\n",
        "import os, re, json, math, random\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.covariance import LedoitWolf\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "np.random.seed(SEED); random.seed(SEED)\n",
        "\n",
        "def set_single_thread():\n",
        "    for var in [\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\"]:\n",
        "        os.environ[var] = \"1\"\n",
        "set_single_thread()\n",
        "\n",
        "print(\"Environment ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATASET_NAME = \"cardiffnlp/tweet_topic_single\"   # 6-label single-label task\n",
        "\n",
        "EMBEDDER_CANDIDATES = [\n",
        "    \"intfloat/multilingual-e5-base\",\n",
        "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
        "]\n",
        "BATCH_SIZE = 64\n",
        "SHOW_PROGRESS = True\n",
        "\n",
        "# TTA\n",
        "ENABLE_TTA = True\n",
        "TTA_VARIANTS = [\"light\", \"heavy\", \"nohashtag\"]\n",
        "\n",
        "# Prototypes\n",
        "USE_MAHALANOBIS = True\n",
        "TRIM_FRAC = 0.10\n",
        "BOOTSTRAP_CENTROIDS = 3\n",
        "\n",
        "# Logistic Regression\n",
        "LR_MAX_ITER = 1500\n",
        "LR_C = 1.0\n",
        "LR_CLASS_WEIGHT = \"balanced\"\n",
        "\n",
        "# kNN\n",
        "KNN_K = 5\n",
        "\n",
        "# Calibration & Open-set\n",
        "CALIBRATION_METHOD = \"sigmoid\"\n",
        "ENABLE_OPEN_SET = True\n",
        "OPEN_SET_STRATEGY = \"per_class\"\n",
        "TARGET_MIN_PRECISION = 0.80\n",
        "\n",
        "# Ensemble grid\n",
        "ENSEMBLE_GRID = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "# Domain priors (optional)\n",
        "DOMAIN_PRIORS = {}\n",
        "\n",
        "# Artifacts\n",
        "ARTIFACT_DIR = \"./artifacts_v3\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "print(\"Config ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset loading\n",
        "\n",
        "def discover_splits(ds: DatasetDict):\n",
        "    keys = list(ds.keys())\n",
        "    prefer = [\n",
        "        (\"train_coling2022\", \"validation_coling2022\", \"test_coling2022\"),\n",
        "        (\"train_coling2022_random\", \"validation_coling2022_random\", \"test_coling2022_random\"),\n",
        "        (\"train\", \"validation\", \"test\"),\n",
        "    ]\n",
        "    for tr, va, te in prefer:\n",
        "        if tr in keys and te in keys:\n",
        "            return tr, (va if va in keys else None), te\n",
        "    tr = \"train\" if \"train\" in keys else keys[0]\n",
        "    te = \"test\" if \"test\" in keys else keys[-1]\n",
        "    va = \"validation\" if \"validation\" in keys else None\n",
        "    return tr, va, te\n",
        "\n",
        "print(\"Loading dataset:\", DATASET_NAME)\n",
        "ds = load_dataset(DATASET_NAME)\n",
        "train_key, val_key, test_key = discover_splits(ds)\n",
        "train_ds, test_ds = ds[train_key], ds[test_key]\n",
        "val_ds = ds[val_key] if val_key else None\n",
        "\n",
        "label_field = \"label\" if \"label\" in train_ds.features else (\"labels\" if \"labels\" in train_ds.features else None)\n",
        "text_field = \"text\"\n",
        "label_names = train_ds.features[label_field].names if label_field == \"label\" and hasattr(train_ds.features[label_field], \"names\") else None\n",
        "\n",
        "print(f\"Splits: train={train_key}, val={val_key}, test={test_key}\")\n",
        "print(\"Labels:\", label_names if label_names else \"multi-label or not provided\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing + TTA cleaners\n",
        "import emoji\n",
        "from wordsegment import load as ws_load, segment as ws_segment\n",
        "ws_load()\n",
        "\n",
        "def split_hashtag(ht: str) -> str:\n",
        "    if not ht or ht[0] != \"#\": return ht\n",
        "    w = ht[1:]\n",
        "    try: return \" \".join(ws_segment(w))\n",
        "    except: return w\n",
        "\n",
        "def clean_light(t: str) -> str:\n",
        "    t = str(t)\n",
        "    t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n",
        "    t = re.sub(r\"@\\w+\",\" \", t)\n",
        "    t = re.sub(r\"#\\w+\", lambda m: \" \" + split_hashtag(m.group()), t)\n",
        "    t = emoji.replace_emoji(t, replace=\" \")\n",
        "    t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n",
        "    t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n",
        "    return re.sub(r\"\\s+\",\" \", t).strip()\n",
        "\n",
        "def clean_heavy(t: str) -> str:\n",
        "    t = clean_light(t)\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"\\b(rt)\\b\",\" \", t)\n",
        "    return re.sub(r\"\\s+\",\" \", t).strip()\n",
        "\n",
        "def clean_nohashtag(t: str) -> str:\n",
        "    t = str(t)\n",
        "    t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n",
        "    t = re.sub(r\"@\\w+\",\" \", t)\n",
        "    t = re.sub(r\"#\\w+\",\" \", t)\n",
        "    t = emoji.replace_emoji(t, replace=\" \")\n",
        "    t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n",
        "    t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n",
        "    return re.sub(r\"\\s+\",\" \", t).strip()\n",
        "\n",
        "CLEANERS = {\"light\": clean_light, \"heavy\": clean_heavy, \"nohashtag\": clean_nohashtag}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "\n",
        "def ds_to_df(d):\n",
        "    return d.to_pandas()\n",
        "\n",
        "df_train, df_test = ds_to_df(train_ds), ds_to_df(test_ds)\n",
        "df_val = ds_to_df(val_ds) if val_ds is not None else None\n",
        "\n",
        "is_multi_label = (label_field == \"labels\")\n",
        "if is_multi_label:\n",
        "    def indices_from_multihot(vec): return [i for i, v in enumerate(vec) if int(v) == 1]\n",
        "    df_train[\"labels_idx\"] = df_train[label_field].apply(indices_from_multihot)\n",
        "    df_test[\"labels_idx\"] = df_test[label_field].apply(indices_from_multihot)\n",
        "    if df_val is not None:\n",
        "        df_val[\"labels_idx\"] = df_val[label_field].apply(indices_from_multihot)\n",
        "else:\n",
        "    if \"label_name\" not in df_train.columns and label_names is not None:\n",
        "        df_train[\"label_name\"] = df_train[\"label\"].apply(lambda i: label_names[i])\n",
        "        df_test[\"label_name\"]  = df_test[\"label\"].apply(lambda i: label_names[i])\n",
        "        if df_val is not None:\n",
        "            df_val[\"label_name\"] = df_val[\"label\"].apply(lambda i: label_names[i])\n",
        "\n",
        "print(\"Train size:\", len(df_train), \"Test size:\", len(df_test), \"Val size:\", (0 if df_val is None else len(df_val)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding utilities\n",
        "\n",
        "def get_embedder(name: str):\n",
        "    return SentenceTransformer(name)\n",
        "\n",
        "def encode_texts(model, texts: List[str], tta_variants=None, batch_size=64, show_progress=True) -> np.ndarray:\n",
        "    if not tta_variants:\n",
        "        tta_variants = [\"light\"]\n",
        "    embs = None\n",
        "    for var in tta_variants:\n",
        "        cleaner = CLEANERS.get(var, clean_light)\n",
        "        cleaned = [cleaner(t) for t in texts]\n",
        "        E = model.encode(cleaned, batch_size=batch_size, show_progress_bar=show_progress,\n",
        "                         convert_to_numpy=True, normalize_embeddings=True)\n",
        "        embs = E if embs is None else (embs + E)\n",
        "    embs = embs / float(len(tta_variants))\n",
        "    return embs\n",
        "\n",
        "def single_label_vectors(df, model, tta=True):\n",
        "    texts = df[text_field].astype(str).tolist()\n",
        "    X = encode_texts(model, texts, tta_variants=(TTA_VARIANTS if (tta and ENABLE_TTA) else [\"light\"]),\n",
        "                     batch_size=BATCH_SIZE, show_progress=SHOW_PROGRESS)\n",
        "    if \"label_name\" in df.columns:\n",
        "        y = np.array(df[\"label_name\"].tolist())\n",
        "    else:\n",
        "        y = np.array(df[\"label\"].tolist())\n",
        "        if not isinstance(y[0], str) and label_names is not None:\n",
        "            y = np.array([label_names[i] for i in y])\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional label descriptions (for prompt blending)\n",
        "DEFAULT_LABEL_PROMPTS = {\n",
        "    \"arts & culture\": \"Arts culture literature museums theater painting design creativity art exhibitions books festivals\",\n",
        "    \"business & entrepreneurs\": \"Business startups markets finance economy entrepreneurship investing commerce companies profits\",\n",
        "    \"pop culture\": \"Celebrities movies TV music fandom memes entertainment viral trends pop culture gossip\",\n",
        "    \"daily life\": \"Everyday life personal updates family friends work routine feelings opinions misc general chatter\",\n",
        "    \"sports & gaming\": \"Sports cricket football basketball olympics esports gaming matches tournaments teams players\",\n",
        "    \"science & technology\": \"Technology science research engineering gadgets software AI data innovation programming internet\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prototypes & utilities\n",
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "\n",
        "def trimmed_centroid(X: np.ndarray, frac=0.10):\n",
        "    if frac <= 0 or frac >= 0.5:\n",
        "        return normalize(X.mean(axis=0, keepdims=True))[0]\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    d = 1 - (normalize(X) @ normalize(mu).T).ravel()  # cosine distance\n",
        "    keep = int(max(1, math.ceil((1-frac)*len(X))))\n",
        "    idx = np.argsort(d)[:keep]\n",
        "    c = normalize(X[idx].mean(axis=0, keepdims=True))[0]\n",
        "    return c\n",
        "\n",
        "\n",
        "def compute_centroids_trimmed(X: np.ndarray, y: np.ndarray, labels: List[str], trim_frac=0.1) -> np.ndarray:\n",
        "    cents = []\n",
        "    for lab in labels:\n",
        "        Xi = X[y == lab]\n",
        "        cents.append(trimmed_centroid(Xi, frac=trim_frac))\n",
        "    cents = np.vstack(cents)\n",
        "    return normalize(cents)\n",
        "\n",
        "\n",
        "def compute_mahalanobis_params(X: np.ndarray, y: np.ndarray, labels: List[str]):\n",
        "    means, precisions = [], []\n",
        "    for lab in labels:\n",
        "        Xi = X[y == lab]\n",
        "        means.append(Xi.mean(axis=0))\n",
        "        if Xi.shape[0] > 2:\n",
        "            lw = LedoitWolf().fit(Xi)\n",
        "            precisions.append(lw.precision_)\n",
        "        else:\n",
        "            precisions.append(np.eye(X.shape[1]))\n",
        "    return np.vstack(means), precisions\n",
        "\n",
        "\n",
        "def cosine_logits(X, centroids):\n",
        "    return X @ centroids.T\n",
        "\n",
        "\n",
        "def mahalanobis_logits(X, means, precisions):\n",
        "    sims = np.zeros((X.shape[0], means.shape[0]))\n",
        "    for k in range(means.shape[0]):\n",
        "        diff = X - means[k]\n",
        "        md2 = np.sum(diff @ precisions[k] * diff, axis=1)\n",
        "        sims[:, k] = -md2\n",
        "    return sims\n",
        "\n",
        "\n",
        "def temperature_scale_logits(logits: np.ndarray, T: float) -> np.ndarray:\n",
        "    return logits / max(T, 1e-6)\n",
        "\n",
        "\n",
        "def softmax(L):\n",
        "    S = L - L.max(axis=1, keepdims=True)\n",
        "    e = np.exp(S)\n",
        "    return e / e.sum(axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training for one embedder\n",
        "\n",
        "def run_for_embedder(embedder_name: str, save_prefix: str = \"run\"):\n",
        "    print(f\"\\n=== Embedder: {embedder_name} ===\")\n",
        "    model = get_embedder(embedder_name)\n",
        "\n",
        "    Xtr, ytr = single_label_vectors(df_train, model, tta=True)\n",
        "    Xte, yte = single_label_vectors(df_test,  model, tta=True)\n",
        "    if df_val is not None:\n",
        "        Xva, yva = single_label_vectors(df_val, model, tta=True)\n",
        "    else:\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.15, stratify=ytr, random_state=42)\n",
        "\n",
        "    labels = sorted(list(set(ytr)))\n",
        "    lab_to_idx = {l:i for i,l in enumerate(labels)}\n",
        "\n",
        "    # Centroids (trimmed + bagging)\n",
        "    if BOOTSTRAP_CENTROIDS > 1:\n",
        "        cents_list = []\n",
        "        for b in range(BOOTSTRAP_CENTROIDS):\n",
        "            idx = np.random.RandomState(42+b).choice(len(Xtr), len(Xtr), replace=True)\n",
        "            cents_list.append(compute_centroids_trimmed(Xtr[idx], ytr[idx], labels, trim_frac=TRIM_FRAC))\n",
        "        centroids = normalize(np.mean(np.stack(cents_list, axis=0), axis=0))\n",
        "    else:\n",
        "        centroids = compute_centroids_trimmed(Xtr, ytr, labels, trim_frac=TRIM_FRAC)\n",
        "\n",
        "    # Optional: blend prompts\n",
        "    prompt_texts = [DEFAULT_LABEL_PROMPTS.get(l, l) for l in labels]\n",
        "    P_emb = encode_texts(model, prompt_texts, tta_variants=[\"heavy\"], batch_size=64, show_progress=False)\n",
        "    P_emb = normalize(P_emb)\n",
        "    centroids = normalize((1-0.2)*centroids + 0.2*P_emb)\n",
        "\n",
        "    # Choose logits\n",
        "    if USE_MAHALANOBIS:\n",
        "        means, precisions = compute_mahalanobis_params(Xtr, ytr, labels)\n",
        "        logits_tr = mahalanobis_logits(Xtr, means, precisions)\n",
        "        logits_va = mahalanobis_logits(Xva, means, precisions)\n",
        "        logits_te = mahalanobis_logits(Xte, means, precisions)\n",
        "    else:\n",
        "        means, precisions = None, None\n",
        "        logits_tr = cosine_logits(Xtr, centroids)\n",
        "        logits_va = cosine_logits(Xva, centroids)\n",
        "        logits_te = cosine_logits(Xte, centroids)\n",
        "\n",
        "    # Temperature scale (single T)\n",
        "    def nll(T):\n",
        "        P = softmax(temperature_scale_logits(logits_va, T))\n",
        "        y_idx = np.array([lab_to_idx[v] for v in yva])\n",
        "        return -np.log(P[np.arange(len(y_idx)), y_idx] + 1e-9).mean()\n",
        "    res = minimize_scalar(nll, bounds=(0.2, 5.0), method=\"bounded\")\n",
        "    T_cent = float(res.x)\n",
        "\n",
        "    P_tr_cent = softmax(temperature_scale_logits(logits_tr, T_cent))\n",
        "    P_va_cent = softmax(temperature_scale_logits(logits_va, T_cent))\n",
        "    P_te_cent = softmax(temperature_scale_logits(logits_te, T_cent))\n",
        "\n",
        "    # Logistic Regression + calibration\n",
        "    lr = LogisticRegression(max_iter=LR_MAX_ITER, solver=\"saga\", C=LR_C, penalty=\"l2\",\n",
        "                            class_weight=LR_CLASS_WEIGHT, random_state=42)\n",
        "    cal = CalibratedClassifierCV(lr, method=CALIBRATION_METHOD, cv=5)\n",
        "    cal.fit(Xtr, ytr)\n",
        "    P_va_lr = cal.predict_proba(Xva)\n",
        "    P_te_lr = cal.predict_proba(Xte)\n",
        "    lr_classes = list(cal.classes_)\n",
        "    cols = [lr_classes.index(l) for l in labels]\n",
        "    P_va_lr, P_te_lr = P_va_lr[:, cols], P_te_lr[:, cols]\n",
        "\n",
        "    # kNN (cosine)\n",
        "    nbrs = NearestNeighbors(n_neighbors=min(KNN_K, len(Xtr)), metric=\"cosine\")\n",
        "    nbrs.fit(Xtr)\n",
        "    dist_va, idx_va = nbrs.kneighbors(Xva, return_distance=True)\n",
        "    dist_te, idx_te = nbrs.kneighbors(Xte, return_distance=True)\n",
        "\n",
        "    def knn_probs(idx_mat, dist_mat):\n",
        "        sim = 1.0 - dist_mat\n",
        "        probs = np.zeros((idx_mat.shape[0], len(labels)), dtype=float)\n",
        "        for i in range(idx_mat.shape[0]):\n",
        "            labs = ytr[idx_mat[i]]\n",
        "            weights = sim[i] / (sim[i].sum() + 1e-9)\n",
        "            for lab, w in zip(labs, weights):\n",
        "                probs[i, lab_to_idx[lab]] += w\n",
        "        return probs\n",
        "\n",
        "    P_va_knn = knn_probs(idx_va, dist_va)\n",
        "    P_te_knn = knn_probs(idx_te, dist_te)\n",
        "\n",
        "    # Ensemble search\n",
        "    best = None\n",
        "    for a in ENSEMBLE_GRID:\n",
        "        for b in ENSEMBLE_GRID:\n",
        "            c = 1.0 - a - b\n",
        "            if c < 0 or c > 1: continue\n",
        "            P_va = a*P_va_cent + b*P_va_lr + c*P_va_knn\n",
        "            pred = [labels[i] for i in np.argmax(P_va, axis=1)]\n",
        "            f1m = f1_score(yva, pred, average=\"macro\")\n",
        "            if best is None or f1m > best[0]:\n",
        "                best = (f1m, (a,b,c))\n",
        "    ens_weights = best[1] if best else (1.0, 0.0, 0.0)\n",
        "\n",
        "    a,b,c = ens_weights\n",
        "    P_te = a*P_te_cent + b*P_te_lr + c*P_te_knn\n",
        "    P_va = a*P_va_cent + b*P_va_lr + c*P_va_knn\n",
        "\n",
        "    # Domain priors (optional)\n",
        "    if DOMAIN_PRIORS:\n",
        "        prior_vec = np.array([DOMAIN_PRIORS.get(l, 1.0) for l in labels], dtype=float)\n",
        "        prior_vec = prior_vec / prior_vec.sum()\n",
        "        P_va = (P_va * prior_vec) / (P_va * prior_vec).sum(axis=1, keepdims=True)\n",
        "        P_te = (P_te * prior_vec) / (P_te * prior_vec).sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Per-class thresholds on validation\n",
        "    thresholds = {}\n",
        "    pred_va = np.argmax(P_va, axis=1)\n",
        "    pred_lab_va = np.array([labels[i] for i in pred_va])\n",
        "    maxP_va = P_va.max(axis=1)\n",
        "    for l in labels:\n",
        "        m = pred_lab_va == l\n",
        "        if m.sum() == 0:\n",
        "            thresholds[l] = 0.0\n",
        "            continue\n",
        "        idx = np.argsort(-maxP_va[m])\n",
        "        probs = maxP_va[m][idx]\n",
        "        correct = (yva[m][idx] == l).astype(int)\n",
        "        tp = np.cumsum(correct)\n",
        "        fp = np.cumsum(1-correct)\n",
        "        precision = tp / np.maximum(tp+fp, 1)\n",
        "        meet = np.where(precision >= TARGET_MIN_PRECISION)[0]\n",
        "        thr = probs[meet[-1]] if len(meet) else probs.max()\n",
        "        thresholds[l] = float(thr)\n",
        "\n",
        "    # Apply thresholds on test\n",
        "    pred_te_idx = np.argmax(P_te, axis=1)\n",
        "    pred_te = np.array([labels[i] for i in pred_te_idx])\n",
        "    maxP_te = P_te.max(axis=1)\n",
        "    for i in range(len(pred_te)):\n",
        "        l = pred_te[i]\n",
        "        if maxP_te[i] < thresholds.get(l, 0.0):\n",
        "            pred_te[i] = \"Other\"\n",
        "\n",
        "    acc = accuracy_score(yte, pred_te)\n",
        "    macro = f1_score(yte, pred_te, average=\"macro\", labels=[l for l in labels if l != \"Other\"])\n",
        "    micro = f1_score(yte, pred_te, average=\"micro\", labels=[l for l in labels if l != \"Other\"])\n",
        "\n",
        "    print(f\"Ensemble weights (Centroid, LR, kNN): {ens_weights}\")\n",
        "    print(f\"Accuracy: {acc:.4f} | F1-macro: {macro:.4f} | F1-micro: {micro:.4f}\")\n",
        "    print(classification_report(yte, pred_te, digits=4))\n",
        "\n",
        "    # Save artifacts\n",
        "    run_dir = os.path.join(ARTIFACT_DIR, save_prefix)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    meta = {\n",
        "        \"embedder\": embedder_name,\n",
        "        \"labels\": labels,\n",
        "        \"ensemble_weights\": ens_weights,\n",
        "        \"use_mahalanobis\": bool(USE_MAHALANOBIS),\n",
        "        \"trim_frac\": TRIM_FRAC,\n",
        "        \"bootstrap_centroids\": BOOTSTRAP_CENTROIDS,\n",
        "        \"tta\": bool(ENABLE_TTA),\n",
        "        \"tta_variants\": TTA_VARIANTS,\n",
        "        \"calibration_method\": CALIBRATION_METHOD,\n",
        "        \"open_set\": bool(ENABLE_OPEN_SET),\n",
        "        \"open_set_strategy\": OPEN_SET_STRATEGY,\n",
        "        \"per_class_thresholds\": thresholds,\n",
        "        \"target_min_precision\": TARGET_MIN_PRECISION,\n",
        "        \"label_prompt_blend\": 0.2,\n",
        "        \"domain_priors\": DOMAIN_PRIORS,\n",
        "    }\n",
        "    with open(os.path.join(run_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    if USE_MAHALANOBIS:\n",
        "        np.savez(os.path.join(run_dir, \"prototypes_mahalanobis.npz\"), means=means, precisions=np.array(precisions, dtype=object))\n",
        "    else:\n",
        "        np.savez(os.path.join(run_dir, \"prototypes_cosine.npz\"), centroids=centroids)\n",
        "\n",
        "    try:\n",
        "        import joblib\n",
        "        joblib.dump(cal, os.path.join(run_dir, \"calibrated_lr.joblib\"))\n",
        "    except Exception as e:\n",
        "        print(\"Note: could not save calibrated LR:\", e)\n",
        "\n",
        "    np.save(os.path.join(run_dir, \"Xtr.npy\"), Xtr)\n",
        "    np.save(os.path.join(run_dir, \"ytr.npy\"), ytr)\n",
        "    np.save(os.path.join(run_dir, \"P_val.npy\"), P_va)\n",
        "    np.save(os.path.join(run_dir, \"y_val.npy\"), yva)\n",
        "\n",
        "    return {\"embedder\": embedder_name, \"acc\": acc, \"f1_macro\": macro, \"f1_micro\": micro, \"run_dir\": run_dir}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run across embedders and pick the best\n",
        "results = []\n",
        "for i, emb in enumerate(EMBEDDER_CANDIDATES):\n",
        "    res = run_for_embedder(emb, save_prefix=f\"run_{i}\")\n",
        "    results.append(res)\n",
        "\n",
        "res_df = pd.DataFrame(results).sort_values([\"f1_macro\",\"acc\"], ascending=[False, False])\n",
        "print(res_df)\n",
        "best_dir = res_df.iloc[0][\"run_dir\"]\n",
        "print(\"Best artifacts at:\", best_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference utility (with exemplars)\n",
        "\n",
        "def predict_texts(texts: List[str], run_dir: str, topk=3, k_exemplar=0):\n",
        "    with open(os.path.join(run_dir, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        meta = json.load(f)\n",
        "    labels = meta[\"labels\"]\n",
        "    embedder = SentenceTransformer(meta[\"embedder\"])\n",
        "\n",
        "    # Load prototypes\n",
        "    cos_path = os.path.join(run_dir, \"prototypes_cosine.npz\")\n",
        "    maha_path = os.path.join(run_dir, \"prototypes_mahalanobis.npz\")\n",
        "    use_maha = os.path.exists(maha_path)\n",
        "    if use_maha:\n",
        "        dat = np.load(maha_path, allow_pickle=True); means = dat[\"means\"]; precisions = list(dat[\"precisions\"])\n",
        "    else:\n",
        "        dat = np.load(cos_path); centroids = dat[\"centroids\"]\n",
        "\n",
        "    # Load LR\n",
        "    import joblib\n",
        "    cal = None\n",
        "    try:\n",
        "        cal = joblib.load(os.path.join(run_dir, \"calibrated_lr.joblib\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Encode with same TTA\n",
        "    tta_vars = meta.get(\"tta_variants\", [\"light\"]) if meta.get(\"tta\", False) else [\"light\"]\n",
        "    def _encode(model, texts, tta_variants):\n",
        "        import emoji, re\n",
        "        from wordsegment import load as ws_load, segment as ws_segment\n",
        "        ws_load()\n",
        "        def split_hashtag(ht: str) -> str:\n",
        "            if not ht or ht[0] != \"#\": return ht\n",
        "            w = ht[1:]\n",
        "            try: return \" \".join(ws_segment(w))\n",
        "            except: return w\n",
        "        def clean_light(t: str) -> str:\n",
        "            t = str(t)\n",
        "            t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n",
        "            t = re.sub(r\"@\\w+\",\" \", t)\n",
        "            t = re.sub(r\"#\\w+\", lambda m: \" \" + split_hashtag(m.group()), t)\n",
        "            t = emoji.replace_emoji(t, replace=\" \")\n",
        "            t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n",
        "            t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n",
        "            return re.sub(r\"\\s+\",\" \", t).strip()\n",
        "        def clean_heavy(t: str) -> str:\n",
        "            t = clean_light(t)\n",
        "            t = t.lower()\n",
        "            t = re.sub(r\"\\b(rt)\\b\",\" \", t)\n",
        "            return re.sub(r\"\\s+\",\" \", t).strip()\n",
        "        def clean_nohashtag(t: str) -> str:\n",
        "            t = str(t)\n",
        "            t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n",
        "            t = re.sub(r\"@\\w+\",\" \", t)\n",
        "            t = re.sub(r\"#\\w+\",\" \", t)\n",
        "            t = emoji.replace_emoji(t, replace=\" \")\n",
        "            t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n",
        "            t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n",
        "            return re.sub(r\"\\s+\",\" \", t).strip()\n",
        "        cleaners = {\"light\": clean_light, \"heavy\": clean_heavy, \"nohashtag\": clean_nohashtag}\n",
        "        embs = None\n",
        "        for var in tta_variants:\n",
        "            cleaner = cleaners.get(var, clean_light)\n",
        "            cleaned = [cleaner(t) for t in texts]\n",
        "            E = model.encode(cleaned, batch_size=64, show_progress_bar=False,\n",
        "                             convert_to_numpy=True, normalize_embeddings=True)\n",
        "            embs = E if embs is None else (embs + E)\n",
        "        embs = embs / float(len(tta_variants))\n",
        "        return embs\n",
        "\n",
        "    X = _encode(embedder, texts, tta_vars)\n",
        "\n",
        "    # Probs from centroid/maha\n",
        "    if use_maha:\n",
        "        sims = np.zeros((X.shape[0], len(labels)))\n",
        "        for k in range(len(labels)):\n",
        "            diff = X - means[k]\n",
        "            md2 = np.sum(diff @ precisions[k] * diff, axis=1)\n",
        "            sims[:, k] = -md2\n",
        "        P_cent = np.exp(sims - sims.max(axis=1, keepdims=True))\n",
        "        P_cent = P_cent / P_cent.sum(axis=1, keepdims=True)\n",
        "    else:\n",
        "        sims = X @ centroids.T\n",
        "        P_cent = np.exp(sims - sims.max(axis=1, keepdims=True))\n",
        "        P_cent = P_cent / P_cent.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # LR probs\n",
        "    P_lr = None\n",
        "    if cal is not None:\n",
        "        P_lr = cal.predict_proba(X)\n",
        "        lr_classes = list(cal.classes_)\n",
        "        cols = [lr_classes.index(l) for l in labels]\n",
        "        P_lr = P_lr[:, cols]\n",
        "\n",
        "    # kNN probs (using saved Xtr,ytr)\n",
        "    P_knn = None\n",
        "    try:\n",
        "        Xtr = np.load(os.path.join(run_dir, \"Xtr.npy\"))\n",
        "        ytr = np.load(os.path.join(run_dir, \"ytr.npy\"), allow_pickle=True)\n",
        "        nbrs = NearestNeighbors(n_neighbors=min(5, len(Xtr)), metric=\"cosine\").fit(Xtr)\n",
        "        dist, idx = nbrs.kneighbors(X, return_distance=True)\n",
        "        sim = 1.0 - dist\n",
        "        lab_to_idx = {l:i for i,l in enumerate(labels)}\n",
        "        P_knn = np.zeros((len(X), len(labels)))\n",
        "        for i in range(len(X)):\n",
        "            labs = ytr[idx[i]]\n",
        "            w = sim[i] / (sim[i].sum() + 1e-9)\n",
        "            for lab, ww in zip(labs, w):\n",
        "                P_knn[i, lab_to_idx[lab]] += ww\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    a,b,c = meta.get(\"ensemble_weights\", [1.0,0.0,0.0])\n",
        "    P = a*P_cent + (b*P_lr if P_lr is not None else 0) + (c*P_knn if P_knn is not None else 0)\n",
        "\n",
        "    # Domain priors\n",
        "    pri = meta.get(\"domain_priors\", {})\n",
        "    if pri:\n",
        "        pv = np.array([pri.get(l,1.0) for l in labels])\n",
        "        pv = pv / pv.sum()\n",
        "        P = (P * pv) / (P * pv).sum(axis=1, keepdims=True)\n",
        "\n",
        "    thresholds = meta.get(\"per_class_thresholds\", None)\n",
        "\n",
        "    idx = np.argsort(-P, axis=1)[:, :topk]\n",
        "    topk_labels = [[labels[j] for j in row] for row in idx]\n",
        "    topk_scores = [[float(P[i, j]) for j in row] for i, row in enumerate(idx)]\n",
        "    pred = [labels[row[0]] for row in idx]\n",
        "\n",
        "    if thresholds:\n",
        "        maxp = P.max(axis=1)\n",
        "        for i in range(len(pred)):\n",
        "            if maxp[i] < thresholds.get(pred[i], 0.0):\n",
        "                pred[i] = \"Other\"\n",
        "\n",
        "    # Exemplars\n",
        "    exemplars = None\n",
        "    if k_exemplar and os.path.exists(os.path.join(run_dir, \"Xtr.npy\")):\n",
        "        Xtr = np.load(os.path.join(run_dir, \"Xtr.npy\"))\n",
        "        ytr = np.load(os.path.join(run_dir, \"ytr.npy\"), allow_pickle=True)\n",
        "        sim = X @ Xtr.T\n",
        "        exemplars = []\n",
        "        for i in range(len(texts)):\n",
        "            idk = np.argsort(-sim[i])[:k_exemplar]\n",
        "            exemplars.append([(float(sim[i, j]), str(ytr[j])) for j in idk])\n",
        "\n",
        "    return {\"pred\": pred, \"topk_labels\": topk_labels, \"topk_scores\": topk_scores, \"exemplars\": exemplars}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV inference helper\n",
        "\n",
        "def predict_on_csv(csv_path: str, run_dir: str, text_col=\"text\", sample=None, topk=3, k_exemplar=0):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"CSV not found:\", csv_path); return None\n",
        "    dfu = pd.read_csv(csv_path)\n",
        "    if text_col not in dfu.columns:\n",
        "        for c in dfu.columns:\n",
        "            if c.lower() == \"text\": text_col = c; break\n",
        "    if text_col not in dfu.columns:\n",
        "        print(\"Column not found. Available:\", dfu.columns.tolist()); return None\n",
        "    dfx = dfu[[text_col]].dropna().rename(columns={text_col:\"text\"})\n",
        "    if sample and len(dfx) > sample:\n",
        "        dfx = dfx.sample(sample, random_state=42)\n",
        "    out = predict_texts(dfx[\"text\"].tolist(), run_dir, topk=topk, k_exemplar=k_exemplar)\n",
        "    df_out = dfx.copy()\n",
        "    df_out[\"pred_topic\"] = out[\"pred\"]\n",
        "    df_out[\"topk_topics\"] = [\"; \".join(x) for x in out[\"topk_labels\"]]\n",
        "    df_out[\"topk_scores\"] = [\", \".join([f\"{s:.3f}\" for s in xs]) for xs in out[\"topk_scores\"]]\n",
        "    out_path = os.path.join(ARTIFACT_DIR, \"predictions_user_csv_v3.csv\")\n",
        "    df_out.to_csv(out_path, index=False)\n",
        "    print(\"Saved:\", out_path)\n",
        "    return df_out\n",
        "\n",
        "# Example usage (uncomment and set your CSV path):\n",
        "# USER_CSV = \"/content/your.csv\"\n",
        "# _ = predict_on_csv(USER_CSV, best_dir, text_col=\"text\", sample=500, topk=3, k_exemplar=3)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
