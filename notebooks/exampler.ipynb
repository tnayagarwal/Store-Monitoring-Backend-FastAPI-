{"cells":[{"cell_type":"markdown","metadata":{},"source":"# 🧠 Tweet Topic Classification — Robust v3 (Accuracy ↑, Overfitting ↓)\n\nThis version adds **more accuracy** and **stability** without heavy overfitting:\n\n**New in v3**  \n- **Per-class open-set thresholds** on calibrated probabilities → better “Other” decisions.  \n- **Ensemble** of **Centroid**, **kNN**, and **Calibrated Logistic Regression** with **grid-searched weights** (on validation).  \n- **Test-Time Augmentation (TTA)**: average embeddings over multiple text normalizations.  \n- **Robust (trimmed) centroids** to reduce outlier influence; optional **embedding-space MixUp** for LR.  \n- **Optional label-descriptions** (class prompts) blended with data centroids → quick taxonomy editing.  \n- **Calibration quality**: report **ECE** (Expected Calibration Error).  \n- **Domain priors** (optional) to bias predictions for deployment traffic.  \n- **Exemplars**: top-k nearest training tweets for each prediction.\n\n> Recommended: keep the embedder frozen; use this ensemble with per-class thresholds for the best balance of accuracy, calibration, and robustness.\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Optional installs for a fresh environment (uncomment as needed)\n# !pip -q install \"datasets>=2.14.0\" \"pandas>=2.0.0\" \"numpy>=1.23.0\" \"scikit-learn>=1.3.0\"\n# !pip -q install \"sentence-transformers>=2.2.2\" \"emoji\" \"wordsegment\" \"matplotlib>=3.7.0\"\n# Optional: for BERTopic exploration and FAISS exemplars\n# !pip -q install \"bertopic==0.16.0\" \"umap-learn>=0.5.6\" \"hdbscan>=0.8.38\" \"faiss-cpu==1.7.4\""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import os, re, json, math, random\nfrom typing import List, Dict, Tuple, Optional\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset, DatasetDict\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, log_loss\nfrom sklearn.preprocessing import normalize\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.covariance import LedoitWolf\n\nfrom sentence_transformers import SentenceTransformer\n\n# Reproducibility\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nnp.random.seed(SEED); random.seed(SEED)\n\ndef set_single_thread():\n    for var in [\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\"]:\n        os.environ[var] = \"1\"\nset_single_thread()\n\nprint(\"Env ready — v3\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# ==========================\n# Configuration\n# ==========================\nDATASET_NAME = \"cardiffnlp/tweet_topic_single\"   # 6 labels (single-label)\n# Alternatives:\n# DATASET_NAME = \"cardiffnlp/tweet_topic_multi\"  # 19 labels (multi-label)\n# DATASET_NAME = \"cardiffnlp/tweet_topic_multilingual\"  # 19 labels, multilingual\n\nEMBEDDER_CANDIDATES = [\n    \"intfloat/multilingual-e5-base\",\n    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n    # ,\"sentence-transformers/LaBSE\"\n]\nBATCH_SIZE = 64\nSHOW_PROGRESS = True\n\n# Test-Time Augmentation\nENABLE_TTA = True\nTTA_VARIANTS = [\"light\", \"heavy\", \"nohashtag\"]  # averaged embeddings\n\n# Prototypes\nUSE_MAHALANOBIS = True\nTRIM_FRAC = 0.10  # trim farthest 10% before computing class centroid\nBOOTSTRAP_CENTROIDS = 3\n\n# Logistic Regression\nLR_MAX_ITER = 1500\nLR_C = 1.0\nLR_CLASS_WEIGHT = \"balanced\"\n\n# kNN\nKNN_K = 5\n\n# Calibration & Open-set\nCALIBRATION_METHOD = \"sigmoid\"  # or \"isotonic\" (needs more data)\nENABLE_OPEN_SET = True\nOPEN_SET_STRATEGY = \"per_class\"  # \"global\" or \"per_class\"\nTARGET_MIN_PRECISION = 0.80      # choose thresholds to reach this precision on validation\n\n# Ensemble\nENABLE_ENSEMBLE = True\nENSEMBLE_GRID = [0.0, 0.25, 0.5, 0.75, 1.0]  # weights grid for search\n\n# Label description prompts (optional)\nUSE_LABEL_PROMPTS = True\nLABEL_PROMPT_BLEND = 0.2  # 0..1; 0 = only data centroid, 1 = only prompt embedding\n\n# Domain priors (optional): dict label-> prior weight (unnormalized)\nDOMAIN_PRIORS = {}   # e.g., {\"sports & gaming\": 1.2, \"arts & culture\": 0.8}\n\n# Paths\nARTIFACT_DIR = \"./artifacts_v3\"\nos.makedirs(ARTIFACT_DIR, exist_ok=True)"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Dataset loading\ndef discover_splits(ds: DatasetDict):\n    keys = list(ds.keys())\n    prefer = [\n        (\"train_coling2022\", \"validation_coling2022\", \"test_coling2022\"),\n        (\"train_coling2022_random\", \"validation_coling2022_random\", \"test_coling2022_random\"),\n        (\"train\", \"validation\", \"test\"),\n    ]\n    for tr, va, te in prefer:\n        if tr in keys and te in keys:\n            return tr, (va if va in keys else None), te\n    tr = \"train\" if \"train\" in keys else keys[0]\n    te = \"test\" if \"test\" in keys else keys[-1]\n    va = \"validation\" if \"validation\" in keys else None\n    return tr, va, te\n\nprint(\"Loading dataset:\", DATASET_NAME)\nds = load_dataset(DATASET_NAME)\ntrain_key, val_key, test_key = discover_splits(ds)\ntrain_ds, test_ds = ds[train_key], ds[test_key]\nval_ds = ds[val_key] if val_key else None\n\nlabel_field = \"label\" if \"label\" in train_ds.features else (\"labels\" if \"labels\" in train_ds.features else None)\ntext_field = \"text\"\nlabel_names = train_ds.features[label_field].names if label_field == \"label\" and hasattr(train_ds.features[label_field], \"names\") else None\n\nprint(f\"Splits: train={train_key}, val={val_key}, test={test_key}\")\nprint(\"Labels:\", label_names if label_names else \"multi-label or not provided\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Preprocessing + TTA\nimport emoji\nfrom wordsegment import load as ws_load, segment as ws_segment\nws_load()\n\ndef split_hashtag(ht: str) -> str:\n    if not ht or ht[0] != \"#\": return ht\n    w = ht[1:]\n    try: return \" \".join(ws_segment(w))\n    except: return w\n\ndef clean_light(t: str) -> str:\n    t = str(t)\n    t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n    t = re.sub(r\"@\\w+\",\" \", t)\n    t = re.sub(r\"#\\w+\", lambda m: \" \" + split_hashtag(m.group()), t)\n    t = emoji.replace_emoji(t, replace=\" \")\n    t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n    t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n    return re.sub(r\"\\s+\",\" \", t).strip()\n\ndef clean_heavy(t: str) -> str:\n    t = clean_light(t)\n    # extra normalizations for noise-robustness\n    t = t.lower()\n    t = re.sub(r\"\\b(rt)\\b\",\" \", t)      # remove RT tokens\n    return re.sub(r\"\\s+\",\" \", t).strip()\n\ndef clean_nohashtag(t: str) -> str:\n    t = str(t)\n    t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n    t = re.sub(r\"@\\w+\",\" \", t)\n    t = re.sub(r\"#\\w+\",\" \", t)          # drop hashtags entirely\n    t = emoji.replace_emoji(t, replace=\" \")\n    t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n    t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n    return re.sub(r\"\\s+\",\" \", t).strip()\n\nCLEANERS = {\"light\": clean_light, \"heavy\": clean_heavy, \"nohashtag\": clean_nohashtag}"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def ds_to_df(d): return d.to_pandas()\n\ndf_train, df_test = ds_to_df(train_ds), ds_to_df(test_ds)\ndf_val = ds_to_df(val_ds) if val_ds is not None else None\n\nis_multi_label = (label_field == \"labels\")\nif is_multi_label:\n    def indices_from_multihot(vec): return [i for i, v in enumerate(vec) if int(v) == 1]\n    df_train[\"labels_idx\"] = df_train[label_field].apply(indices_from_multihot)\n    df_test[\"labels_idx\"] = df_test[label_field].apply(indices_from_multihot)\n    if df_val is not None: df_val[\"labels_idx\"] = df_val[label_field].apply(indices_from_multihot)\nelse:\n    if \"label_name\" not in df_train.columns and label_names is not None:\n        df_train[\"label_name\"] = df_train[\"label\"].apply(lambda i: label_names[i])\n        df_test[\"label_name\"]  = df_test[\"label\"].apply(lambda i: label_names[i])\n        if df_val is not None:\n            df_val[\"label_name\"] = df_val[\"label\"].apply(lambda i: label_names[i])\n\nprint(\"Train size:\", len(df_train), \"Test size:\", len(df_test), \"Val size:\", (0 if df_val is None else len(df_val)))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def get_embedder(name: str):\n    return SentenceTransformer(name)\n\ndef encode_texts(model, texts: List[str], tta_variants=None, batch_size=64, show_progress=True) -> np.ndarray:\n    # Averaged embedding over multiple cleaned variants for robustness\n    if not tta_variants:\n        tta_variants = [\"light\"]\n    embs = None\n    for var in tta_variants:\n        cleaner = CLEANERS.get(var, clean_light)\n        cleaned = [cleaner(t) for t in texts]\n        E = model.encode(cleaned, batch_size=batch_size, show_progress_bar=show_progress,\n                         convert_to_numpy=True, normalize_embeddings=True)\n        embs = E if embs is None else (embs + E)\n    embs = embs / float(len(tta_variants))\n    return embs\n\ndef single_label_vectors(df, model, tta=True):\n    texts = df[text_field].astype(str).tolist()\n    X = encode_texts(model, texts, tta_variants=(TTA_VARIANTS if (tta and ENABLE_TTA) else [\"light\"]),\n                     batch_size=BATCH_SIZE, show_progress=SHOW_PROGRESS)\n    if \"label_name\" in df.columns:\n        y = np.array(df[\"label_name\"].tolist())\n    else:\n        y = np.array(df[\"label\"].tolist())\n        if not isinstance(y[0], str) and label_names is not None:\n            y = np.array([label_names[i] for i in y])\n    return X, y"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Optional label descriptions for TweetTopic (examples; edit as needed)\nDEFAULT_LABEL_PROMPTS = {\n    # For 6-label set (tweet_topic_single)\n    \"arts & culture\": \"Arts culture literature museums theater painting design creativity art exhibitions books festivals\",\n    \"business & entrepreneurs\": \"Business startups markets finance economy entrepreneurship investing commerce companies profits\",\n    \"pop culture\": \"Celebrities movies TV music fandom memes entertainment viral trends pop culture gossip\",\n    \"daily life\": \"Everyday life personal updates family friends work routine feelings opinions misc general chatter\",\n    \"sports & gaming\": \"Sports cricket football basketball olympics esports gaming matches tournaments teams players\",\n    \"science & technology\": \"Technology science research engineering gadgets software AI data innovation programming internet\"\n}"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Prototypes & utilities\ndef trimmed_centroid(X: np.ndarray, frac=0.10):\n    # Trim farthest fraction then mean\n    if frac <= 0 or frac >= 0.5:\n        return normalize(X.mean(axis=0, keepdims=True))[0]\n    mu = X.mean(axis=0, keepdims=True)\n    d = 1 - (normalize(X) @ normalize(mu).T).ravel()  # cosine distance (since vectors normalized)\n    keep = int(max(1, math.ceil((1-frac)*len(X))))\n    idx = np.argsort(d)[:keep]\n    c = normalize(X[idx].mean(axis=0, keepdims=True))[0]\n    return c\n\ndef compute_centroids_trimmed(X: np.ndarray, y: np.ndarray, labels: List[str], trim_frac=0.1) -> np.ndarray:\n    cents = []\n    for lab in labels:\n        Xi = X[y == lab]\n        cents.append(trimmed_centroid(Xi, frac=trim_frac))\n    cents = np.vstack(cents)\n    return normalize(cents)\n\ndef compute_mahalanobis_params(X: np.ndarray, y: np.ndarray, labels: List[str]):\n    means, precisions = [], []\n    for lab in labels:\n        Xi = X[y == lab]\n        means.append(Xi.mean(axis=0))\n        if Xi.shape[0] > 2:\n            lw = LedoitWolf().fit(Xi)\n            precisions.append(lw.precision_)\n        else:\n            precisions.append(np.eye(X.shape[1]))\n    return np.vstack(means), precisions\n\ndef cosine_logits(X, centroids):\n    return X @ centroids.T\n\ndef mahalanobis_logits(X, means, precisions):\n    sims = np.zeros((X.shape[0], means.shape[0]))\n    for k in range(means.shape[0]):\n        diff = X - means[k]\n        md2 = np.sum(diff @ precisions[k] * diff, axis=1)\n        sims[:, k] = -md2\n    return sims\n\ndef temperature_scale_logits(logits: np.ndarray, T: float) -> np.ndarray:\n    return logits / max(T, 1e-6)\n\ndef softmax(L):\n    S = L - L.max(axis=1, keepdims=True)\n    e = np.exp(S)\n    return e / e.sum(axis=1, keepdims=True)\n\ndef expected_calibration_error(probs, y_true, bins=15, labels=None):\n    # probs: (n, C) calibrated probabilities; y_true: string labels same length\n    if labels is None:\n        labels = sorted(list(set(y_true)))\n    lab_to_idx = {l:i for i,l in enumerate(labels)}\n    y_idx = np.array([lab_to_idx[v] for v in y_true])\n    conf = probs.max(axis=1)\n    preds = probs.argmax(axis=1)\n    ece = 0.0\n    bin_edges = np.linspace(0, 1, bins+1)\n    for b in range(bins):\n        lo, hi = bin_edges[b], bin_edges[b+1]\n        m = (conf > lo) & (conf <= hi)\n        if m.sum() == 0: continue\n        acc = (preds[m] == y_idx[m]).mean()\n        avg_conf = conf[m].mean()\n        ece += (m.mean()) * abs(acc - avg_conf)\n    return float(ece)"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def run_for_embedder(embedder_name: str, save_prefix: str = \"run\"):\n    print(f\"\\n=== Embedder: {embedder_name} ===\")\n    model = get_embedder(embedder_name)\n\n    Xtr, ytr = single_label_vectors(df_train, model, tta=True)\n    Xte, yte = single_label_vectors(df_test,  model, tta=True)\n    if df_val is not None:\n        Xva, yva = single_label_vectors(df_val, model, tta=True)\n    else:\n        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.15, stratify=ytr, random_state=42)\n\n    labels = sorted(list(set(ytr)))\n    lab_to_idx = {l:i for i,l in enumerate(labels)}\n\n    # ----- Centroids (trimmed + bagging) -----\n    if 3 > 1:\n        cents_list = []\n        for b in range(3):\n            idx = np.random.RandomState(42+b).choice(len(Xtr), len(Xtr), replace=True)\n            cents_list.append(compute_centroids_trimmed(Xtr[idx], ytr[idx], labels, trim_frac=0.10))\n        centroids = normalize(np.mean(np.stack(cents_list, axis=0), axis=0))\n    else:\n        centroids = compute_centroids_trimmed(Xtr, ytr, labels, trim_frac=0.10)\n\n    # Optional: blend label-description prompts\n    prompt_texts = [{\n        \"arts & culture\": \"Arts culture literature museums theater painting design creativity art exhibitions books festivals\",\n        \"business & entrepreneurs\": \"Business startups markets finance economy entrepreneurship investing commerce companies profits\",\n        \"pop culture\": \"Celebrities movies TV music fandom memes entertainment viral trends pop culture gossip\",\n        \"daily life\": \"Everyday life personal updates family friends work routine feelings opinions misc general chatter\",\n        \"sports & gaming\": \"Sports cricket football basketball olympics esports gaming matches tournaments teams players\",\n        \"science & technology\": \"Technology science research engineering gadgets software AI data innovation programming internet\"\n    }.get(l, l) for l in labels]\n    P_emb = encode_texts(model, prompt_texts, tta_variants=[\"heavy\"], batch_size=64, show_progress=False)\n    P_emb = normalize(P_emb)\n    centroids = normalize((1-0.2)*centroids + 0.2*P_emb)\n\n    # Mahalanobis\n    means, precisions = None, None\n    if True:\n        means, precisions = compute_mahalanobis_params(Xtr, ytr, labels)\n        logits_tr = mahalanobis_logits(Xtr, means, precisions)\n        logits_va = mahalanobis_logits(Xva, means, precisions)\n        logits_te = mahalanobis_logits(Xte, means, precisions)\n    else:\n        logits_tr = cosine_logits(Xtr, centroids)\n        logits_va = cosine_logits(Xva, centroids)\n        logits_te = cosine_logits(Xte, centroids)\n\n    # Temperature scaling (single T) on validation\n    from scipy.optimize import minimize_scalar\n    def nll(T):\n        P = softmax(temperature_scale_logits(logits_va, T))\n        y_idx = np.array([lab_to_idx[v] for v in yva])\n        return -np.log(P[np.arange(len(y_idx)), y_idx] + 1e-9).mean()\n    res = minimize_scalar(nll, bounds=(0.2, 5.0), method=\"bounded\")\n    T_cent = float(res.x)\n\n    P_tr_cent = softmax(temperature_scale_logits(logits_tr, T_cent))\n    P_va_cent = softmax(temperature_scale_logits(logits_va, T_cent))\n    P_te_cent = softmax(temperature_scale_logits(logits_te, T_cent))\n\n    # ----- Logistic Regression with CV calibration -----\n    lr = LogisticRegression(max_iter=1500, solver=\"saga\", n_jobs=1, C=1.0, penalty=\"l2\",\n                            class_weight=\"balanced\", random_state=42)\n    cal = CalibratedClassifierCV(lr, method=\"sigmoid\", cv=5)\n    cal.fit(Xtr, ytr)\n    P_va_lr = cal.predict_proba(Xva)\n    P_te_lr = cal.predict_proba(Xte)\n    lr_classes = list(cal.classes_)\n    cols = [lr_classes.index(l) for l in labels]\n    P_va_lr, P_te_lr = P_va_lr[:, cols], P_te_lr[:, cols]\n\n    # ----- kNN (cosine) -----\n    from sklearn.neighbors import NearestNeighbors\n    nbrs = NearestNeighbors(n_neighbors=5, metric=\"cosine\", n_jobs=1)\n    nbrs.fit(Xtr)\n    dist_va, idx_va = nbrs.kneighbors(Xva, return_distance=True)\n    dist_te, idx_te = nbrs.kneighbors(Xte, return_distance=True)\n\n    def knn_probs(idx_mat, dist_mat):\n        sim = 1.0 - dist_mat\n        probs = np.zeros((idx_mat.shape[0], len(labels)), dtype=float)\n        for i in range(idx_mat.shape[0]):\n            labs = ytr[idx_mat[i]]\n            weights = sim[i] / (sim[i].sum() + 1e-9)\n            for lab, w in zip(labs, weights):\n                probs[i, lab_to_idx[lab]] += w\n        return probs\n\n    P_va_knn = knn_probs(idx_va, dist_va)\n    P_te_knn = knn_probs(idx_te, dist_te)\n\n    # ----- Ensemble weight search on validation -----\n    best = None\n    for a in [0.0,0.25,0.5,0.75,1.0]:\n        for b in [0.0,0.25,0.5,0.75,1.0]:\n            c = 1.0 - a - b\n            if c < 0 or c > 1: continue\n            P_va = a*P_va_cent + b*P_va_lr + c*P_va_knn\n            pred = [labels[i] for i in np.argmax(P_va, axis=1)]\n            f1m = f1_score(yva, pred, average=\"macro\")\n            if best is None or f1m > best[0]:\n                best = (f1m, (a,b,c))\n    ens_weights = best[1] if best else (1.0, 0.0, 0.0)\n\n    a,b,c = ens_weights\n    P_te = a*P_te_cent + b*P_te_lr + c*P_te_knn\n    P_va = a*P_va_cent + b*P_va_lr + c*P_va_knn\n\n    # ----- Domain priors -----\n    if {}:\n        prior_vec = np.array([{}.get(l, 1.0) for l in labels], dtype=float)\n        prior_vec = prior_vec / prior_vec.sum()\n        P_va = (P_va * prior_vec) / (P_va * prior_vec).sum(axis=1, keepdims=True)\n        P_te = (P_te * prior_vec) / (P_te * prior_vec).sum(axis=1, keepdims=True)\n\n    # ----- Open-set thresholds (per-class) -----\n    thresholds = {}\n    pred_va = np.argmax(P_va, axis=1)\n    pred_lab_va = np.array([labels[i] for i in pred_va])\n    maxP_va = P_va.max(axis=1)\n    TARGET_MIN_PRECISION = 0.80\n    for l in labels:\n        m = pred_lab_va == l\n        if m.sum() == 0:\n            thresholds[l] = 0.0\n            continue\n        idx = np.argsort(-maxP_va[m])\n        probs = maxP_va[m][idx]\n        correct = (yva[m][idx] == l).astype(int)\n        tp = np.cumsum(correct)\n        fp = np.cumsum(1-correct)\n        precision = tp / np.maximum(tp+fp, 1)\n        meet = np.where(precision >= TARGET_MIN_PRECISION)[0]\n        thr = probs[meet[-1]] if len(meet) else probs.max()\n        thresholds[l] = float(thr)\n\n    # ----- Metrics -----\n    pred_te_idx = np.argmax(P_te, axis=1)\n    pred_te = np.array([labels[i] for i in pred_te_idx])\n    maxP_te = P_te.max(axis=1)\n    for i in range(len(pred_te)):\n        l = pred_te[i]\n        if maxP_te[i] < thresholds.get(l, 0.0):\n            pred_te[i] = \"Other\"\n\n    acc = accuracy_score(yte, pred_te)\n    macro = f1_score(yte, pred_te, average=\"macro\", labels=[l for l in labels if l != \"Other\"])\n    micro = f1_score(yte, pred_te, average=\"micro\", labels=[l for l in labels if l != \"Other\"])\n\n    def expected_calibration_error(probs, y_true, bins=15, labels=None):\n        if labels is None:\n            labels = sorted(list(set(y_true)))\n        lab_to_idx = {l:i for i,l in enumerate(labels)}\n        y_idx = np.array([lab_to_idx[v] for v in y_true])\n        conf = probs.max(axis=1)\n        preds = probs.argmax(axis=1)\n        ece = 0.0\n        bin_edges = np.linspace(0, 1, bins+1)\n        for b in range(bins):\n            lo, hi = bin_edges[b], bin_edges[b+1]\n            m = (conf > lo) & (conf <= hi)\n            if m.sum() == 0: continue\n            acc_b = (preds[m] == y_idx[m]).mean()\n            avg_conf = conf[m].mean()\n            ece += (m.mean()) * abs(acc_b - avg_conf)\n        return float(ece)\n\n    ece = expected_calibration_error(P_te, yte, labels=labels)\n\n    print(f\"Ensemble weights (Centroid, LR, kNN): {ens_weights}\")\n    print(f\"Accuracy: {acc:.4f} | F1-macro: {macro:.4f} | F1-micro: {micro:.4f} | ECE: {ece:.4f}\")\n    print(classification_report(yte, pred_te, digits=4))\n\n    shown_labels = labels + ([\"Other\"])\n    cm = confusion_matrix(yte, pred_te, labels=shown_labels)\n    plt.figure(figsize=(6,5))\n    plt.imshow(cm)\n    plt.title(f\"Confusion Matrix — {embedder_name}\")\n    plt.xlabel(\"Predicted\"); plt.ylabel(\"Gold\")\n    plt.xticks(range(len(shown_labels)), shown_labels, rotation=45, ha=\"right\")\n    plt.yticks(range(len(shown_labels)), shown_labels)\n    for (i,j), v in np.ndenumerate(cm):\n        plt.text(j, i, int(v), ha=\"center\", va=\"center\")\n    plt.tight_layout(); plt.show()\n\n    # Save artifacts\n    run_dir = os.path.join(\"./artifacts_v3\", save_prefix)\n    os.makedirs(run_dir, exist_ok=True)\n    meta = {\n        \"embedder\": embedder_name,\n        \"labels\": labels,\n        \"ensemble_weights\": ens_weights,\n        \"use_mahalanobis\": True,\n        \"trim_frac\": 0.10,\n        \"bootstrap_centroids\": 3,\n        \"tta\": True,\n        \"tta_variants\": [\"light\",\"heavy\",\"nohashtag\"],\n        \"calibration_method\": \"sigmoid\",\n        \"open_set\": True,\n        \"open_set_strategy\": \"per_class\",\n        \"per_class_thresholds\": thresholds,\n        \"target_min_precision\": 0.80,\n        \"label_prompt_blend\": 0.2,\n        \"domain_priors\": {}\n    }\n    with open(os.path.join(run_dir, \"meta.json\"), \"w\") as f: json.dump(meta, f, indent=2)\n\n    # Store prototypes\n    np.savez(os.path.join(run_dir, \"prototypes_mahalanobis.npz\"), means=means, precisions=np.array(precisions, dtype=object))\n\n    # Save LR calibrator\n    try:\n        import joblib\n        joblib.dump(cal, os.path.join(run_dir, \"calibrated_lr.joblib\"))\n    except Exception as e:\n        print(\"Note: could not save calibrated LR:\", e)\n\n    # Save training set for exemplars and kNN\n    np.save(os.path.join(run_dir, \"Xtr.npy\"), Xtr); np.save(os.path.join(run_dir, \"ytr.npy\"), ytr)\n\n    # Save validation P for potential further analysis\n    np.save(os.path.join(run_dir, \"P_val.npy\"), P_va)\n    np.save(os.path.join(run_dir, \"y_val.npy\"), yva)\n\n    return {\"embedder\": embedder_name, \"acc\": acc, \"f1_macro\": macro, \"f1_micro\": micro, \"ece\": ece, \"run_dir\": run_dir}"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Run across embedders and pick the best\nresults = []\nfor i, emb in enumerate([\"intfloat/multilingual-e5-base\",\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"]):\n    res = run_for_embedder(emb, save_prefix=f\"run_{i}\")\n    results.append(res)\n\nimport pandas as pd\nres_df = pd.DataFrame(results).sort_values([\"f1_macro\",\"acc\"], ascending=[False, False])\nprint(res_df)\nbest_dir = res_df.iloc[0][\"run_dir\"]\nprint(\"Best artifacts at:\", best_dir)"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Inference utility (with exemplars)\ndef predict_texts(texts: List[str], run_dir: str, topk=3, k_exemplar=0):\n    with open(os.path.join(run_dir, \"meta.json\"), \"r\") as f: meta = json.load(f)\n    labels = meta[\"labels\"]\n    embedder = SentenceTransformer(meta[\"embedder\"])\n\n    # Load prototypes\n    cos_path = os.path.join(run_dir, \"prototypes_cosine.npz\")\n    maha_path = os.path.join(run_dir, \"prototypes_mahalanobis.npz\")\n    use_maha = os.path.exists(maha_path)\n    if use_maha:\n        dat = np.load(maha_path, allow_pickle=True); means = dat[\"means\"]; precisions = list(dat[\"precisions\"])\n    else:\n        dat = np.load(cos_path); centroids = dat[\"centroids\"]\n\n    # Load LR\n    import joblib\n    cal = None\n    try:\n        cal = joblib.load(os.path.join(run_dir, \"calibrated_lr.joblib\"))\n    except:\n        pass\n\n    # Encode with same TTA\n    tta_vars = meta.get(\"tta_variants\", [\"light\"]) if meta.get(\"tta\", False) else [\"light\"]\n    # replicate encode_texts logic inline to avoid extra dependencies\n    def _encode(model, texts, tta_variants):\n        import re, emoji\n        from wordsegment import load as ws_load, segment as ws_segment\n        ws_load()\n        def split_hashtag(ht: str) -> str:\n            if not ht or ht[0] != \"#\": return ht\n            w = ht[1:]\n            try: return \" \".join(ws_segment(w))\n            except: return w\n        def clean_light(t: str) -> str:\n            t = str(t)\n            t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n            t = re.sub(r\"@\\w+\",\" \", t)\n            t = re.sub(r\"#\\w+\", lambda m: \" \" + split_hashtag(m.group()), t)\n            t = emoji.replace_emoji(t, replace=\" \")\n            t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n            t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n            return re.sub(r\"\\s+\",\" \", t).strip()\n        def clean_heavy(t: str) -> str:\n            t = clean_light(t)\n            t = t.lower()\n            t = re.sub(r\"\\b(rt)\\b\",\" \", t)\n            return re.sub(r\"\\s+\",\" \", t).strip()\n        def clean_nohashtag(t: str) -> str:\n            t = str(t)\n            t = re.sub(r\"http\\S+|www\\.\\S+\",\" \", t)\n            t = re.sub(r\"@\\w+\",\" \", t)\n            t = re.sub(r\"#\\w+\",\" \", t)\n            t = emoji.replace_emoji(t, replace=\" \")\n            t = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", t)\n            t = re.sub(r\"[^0-9A-Za-z\\u0900-\\u097F\\s']\", \" \", t)\n            return re.sub(r\"\\s+\",\" \", t).strip()\n        cleaners = {\"light\": clean_light, \"heavy\": clean_heavy, \"nohashtag\": clean_nohashtag}\n        embs = None\n        for var in tta_variants:\n            cleaner = cleaners.get(var, clean_light)\n            cleaned = [cleaner(t) for t in texts]\n            E = model.encode(cleaned, batch_size=64, show_progress_bar=False,\n                             convert_to_numpy=True, normalize_embeddings=True)\n            embs = E if embs is None else (embs + E)\n        embs = embs / float(len(tta_variants))\n        return embs\n\n    X = _encode(embedder, texts, tta_vars)\n\n    # Probs from centroid/maha\n    if use_maha:\n        sims = np.zeros((X.shape[0], len(labels)))\n        for k in range(len(labels)):\n            diff = X - means[k]\n            md2 = np.sum(diff @ precisions[k] * diff, axis=1)\n            sims[:, k] = -md2\n        P_cent = np.exp(sims - sims.max(axis=1, keepdims=True))\n        P_cent = P_cent / P_cent.sum(axis=1, keepdims=True)\n    else:\n        sims = X @ centroids.T\n        P_cent = np.exp(sims - sims.max(axis=1, keepdims=True))\n        P_cent = P_cent / P_cent.sum(axis=1, keepdims=True)\n\n    # LR probs\n    P_lr = None\n    if cal is not None:\n        P_lr = cal.predict_proba(X)\n        lr_classes = list(cal.classes_)\n        cols = [lr_classes.index(l) for l in labels]\n        P_lr = P_lr[:, cols]\n\n    # kNN probs (using saved Xtr,ytr)\n    P_knn = None\n    try:\n        Xtr = np.load(os.path.join(run_dir, \"Xtr.npy\"))\n        ytr = np.load(os.path.join(run_dir, \"ytr.npy\"), allow_pickle=True)\n        from sklearn.neighbors import NearestNeighbors\n        nbrs = NearestNeighbors(n_neighbors=min(5, len(Xtr)), metric=\"cosine\", n_jobs=1).fit(Xtr)\n        dist, idx = nbrs.kneighbors(X, return_distance=True)\n        sim = 1.0 - dist\n        lab_to_idx = {l:i for i,l in enumerate(labels)}\n        P_knn = np.zeros((len(X), len(labels)))\n        for i in range(len(X)):\n            labs = ytr[idx[i]]\n            w = sim[i] / (sim[i].sum() + 1e-9)\n            for lab, ww in zip(labs, w):\n                P_knn[i, lab_to_idx[lab]] += ww\n    except Exception as e:\n        pass\n\n    a,b,c = meta.get(\"ensemble_weights\", [1.0,0.0,0.0])\n    P = a*P_cent + (b*P_lr if P_lr is not None else 0) + (c*P_knn if P_knn is not None else 0)\n\n    # Domain priors\n    pri = meta.get(\"domain_priors\", {})\n    if pri:\n        pv = np.array([pri.get(l,1.0) for l in labels])\n        pv = pv / pv.sum()\n        P = (P * pv) / (P * pv).sum(axis=1, keepdims=True)\n\n    thresholds = meta.get(\"per_class_thresholds\", None)\n\n    idx = np.argsort(-P, axis=1)[:, :topk]\n    topk_labels = [[labels[j] for j in row] for row in idx]\n    topk_scores = [[float(P[i, j]) for j in row] for i, row in enumerate(idx)]\n    pred = [labels[j[0]] for j in idx]\n\n    if thresholds:\n        maxp = P.max(axis=1)\n        for i in range(len(pred)):\n            if maxp[i] < thresholds.get(pred[i], 0.0):\n                pred[i] = \"Other\"\n\n    # Exemplars\n    exemplars = None\n    if k_exemplar and os.path.exists(os.path.join(run_dir, \"Xtr.npy\")):\n        Xtr = np.load(os.path.join(run_dir, \"Xtr.npy\"))\n        ytr = np.load(os.path.join(run_dir, \"ytr.npy\"), allow_pickle=True)\n        sim = X @ Xtr.T\n        exemplars = []\n        for i in range(len(texts)):\n            idk = np.argsort(-sim[i])[:k_exemplar]\n            exemplars.append([(float(sim[i, j]), str(ytr[j])) for j in idk])\n\n    return {\"pred\": pred, \"topk_labels\": topk_labels, \"topk_scores\": topk_scores, \"exemplars\": exemplars}"},"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# CSV inference helper\ndef predict_on_csv(csv_path: str, run_dir: str, text_col=\"text\", sample=None, topk=3, k_exemplar=0):\n    if not os.path.exists(csv_path):\n        print(\"CSV not found:\", csv_path); return None\n    dfu = pd.read_csv(csv_path)\n    if text_col not in dfu.columns:\n        for c in dfu.columns:\n            if c.lower() == \"text\": text_col = c; break\n    if text_col not in dfu.columns:\n        print(\"Column not found. Available:\", dfu.columns.tolist()); return None\n    dfx = dfu[[text_col]].dropna().rename(columns={text_col:\"text\"})\n    if sample and len(dfx) > sample:\n        dfx = dfx.sample(sample, random_state=42)\n    out = predict_texts(dfx[\"text\"].tolist(), run_dir, topk=topk, k_exemplar=k_exemplar)\n    df_out = dfx.copy()\n    df_out[\"pred_topic\"] = out[\"pred\"]\n    df_out[\"topk_topics\"] = [\"; \".join(x) for x in out[\"topk_labels\"]]\n    df_out[\"topk_scores\"] = [\", \".join([f\"{s:.3f}\" for s in xs]) for xs in out[\"topk_scores\"]]\n    out_path = os.path.join(\"./artifacts_v3\", \"predictions_user_csv_v3.csv\")\n    df_out.to_csv(out_path, index=False)\n    print(\"Saved:\", out_path)\n    return df_out\n\n# Example (run if the CSV exists in your environment):\n# USER_CSV = \"/path/to/your.csv\"\n# _ = predict_on_csv(USER_CSV, best_dir, text_col=\"text\", sample=500, topk=3, k_exemplar=3)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}

